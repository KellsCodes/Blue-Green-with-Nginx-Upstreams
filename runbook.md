## Runbook: Blue/Green Deployment Monitoring

**Purpose**
This runbook helps operators understand, interpret, and respond to alerts generated by the Alert Watcher service.
It ensures rapid diagnosis and minimal downtime during failovers or error spikes.

### Overview
| Component         | Description                                                                         |
| ----------------- | ----------------------------------------------------------------------------------- |
| **Nginx**         | Routes traffic between Blue and Green services using upstream pools.                |
| **Alert Watcher** | Monitors Nginx access logs in real time to detect failovers or elevated 5xx errors. |
| **Slack Alerts**  | Delivered via configured webhook to notify operators of issues or recovery events.  |

All configurations are set via .env — no code changes required.

**Environment Variables**
| Variable               | Description                                             | Default                     |
| ---------------------- | ------------------------------------------------------- | --------------------------- |
| `SLACK_WEBHOOK_URL`    | Slack Incoming Webhook URL for posting alerts.          | *(required)*                |
| `LOG_PATH`             | Path to shared Nginx log file.                          | `/var/log/nginx/access.log` |
| `ACTIVE_POOL`          | Initial active pool (`blue` or `green`).                | `blue`                      |
| `ERROR_RATE_THRESHOLD` | Maximum allowed 5xx error percentage before alerting.   | `2`                         |
| `WINDOW_SIZE`          | Number of recent requests to track for error rate.      | `200`                       |
| `ALERT_COOLDOWN_SEC`   | Minimum seconds between repeated alerts for same event. | `300`                       |

**Alert Types & Actions**
1. **Failover Detected**
Failover Event Detected
Traffic switched from blue → green.
Operators should verify upstream health and confirm recovery.

**Meaning**
Nginx began routing requests from one pool to another — typically after a health failure on the active pool.

**Operator Action**
1. Check which service (Blue or Green) is active.
2. Verify container health with:
```bash
docker ps
docker logs blue-app
docker logs green-app
```
3. Inspect /healthz endpoints of both apps.
4. If the failed pool recovers, you may switch traffic back manually using:
```bash
docker exec nginx nginx -s reload
```
5. Continue monitoring Slack for recovery confirmation.

2. **High Error Rate Detected**
```bash
Elevated Error Rate
Pool: blue
Errors: 15/200 (7.5%)
Threshold: 2%
Immediate investigation recommended.
```

**Meaning**
Nginx served an unusually high number of HTTP 5xx responses (e.g., 502, 503, 504) within the last N requests.

**Operator Action**
1. Inspect the active pool’s container:
```bash
docker logs blue-app
```
2. Look for upstream errors (e.g., DB unavailability, app crash, or slow response).
3. If errors persist:
	* Temporarily switch traffic to the healthy pool (Green).
	* Investigate the root cause in the failing service.
4. Once stable, confirm logs show normal responses.

3. **Recovery / Normal Operation**
(Implicit; shown when error rates stabilize and traffic continues smoothly)

**Meaning**
The system resumed healthy operation after transient issues or a pool switch.

**Operator Action**
✅ No immediate action required.
Continue monitoring metrics and ensure no regression occurs.

**Maintenance Mode (Suppressing Alerts)**
During planned deployments or scaling operations, you can suppress alerts to avoid false positives.
1. Set a temporary flag in .env (for example):
```bash
MAINTENANCE_MODE=true
```
2. Restart the watcher service:
```bash
docker-compose restart alert_watcher
```
3. Once maintenance completes, unset the flag and restart again:
```bash
MAINTENANCE_MODE=false
```
docker-compose restart alert_watcher
(Note: This flag is optional; not implemented, simply pause the watcher container during maintenance.)

**Troubleshooting**
| Symptom                  | Possible Cause                         | Fix                                               |
| ------------------------ | -------------------------------------- | ------------------------------------------------- |
| No Slack alerts received | Missing or invalid `SLACK_WEBHOOK_URL` | Verify `.env` and restart watcher                 |
| Duplicate alerts         | Cooldown too short                     | Increase `ALERT_COOLDOWN_SEC`                     |
| “Invalid JSON” logs      | Nginx log format not in JSON           | Ensure Nginx `access_log` uses custom JSON format |
| Frequent failover        | Unstable app health checks             | Inspect `/healthz` endpoints for both pools       |


**Expected Alerts During Testing**
| Scenario                    | Expected Alert              |
| --------------------------- | --------------------------- |
| Baseline Test               | No alerts                   |
| Chaos Injection (kill Blue) | Failover alert (Blue→Green) |
| Error Simulation            | Error-rate alert (>2%)      |

